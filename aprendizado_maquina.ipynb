{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Ambiente (Java + PySpark + SparkSession)\n","\n","# Fecha Spark anterior (se houver)\n","try:\n","    spark.stop()\n","except:\n","    pass\n","\n","# Java + PySpark estáveis para Python 3.12\n","!apt-get update -qq\n","!apt-get install -y openjdk-17-jdk-headless -qq\n","!pip -q install -U pyspark==3.5.1\n","\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n","os.environ[\"PATH\"]  = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","\n","from pyspark.sql import SparkSession\n","spark = (SparkSession.builder\n","         .appName(\"eixo05-preprocess\")\n","         .getOrCreate())\n","print(\"Spark OK ->\", spark.version)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_TJ7tYC5t7E","executionInfo":{"status":"ok","timestamp":1762081142614,"user_tz":0,"elapsed":22911,"user":{"displayName":"Ravi Ferreira Pellizzi","userId":"04678001779669650850"}},"outputId":"1efcfa60-af8d-4ad7-9238-b5a719605a0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Spark OK -> 3.5.1\n"]}]},{"cell_type":"code","source":["# Montar Drive + garantir Spark ativo (sem reinstalar nada)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","from pyspark.sql import SparkSession\n","try:\n","    spark\n","except NameError:\n","    spark = SparkSession.builder.getOrCreate()\n","\n","base_path = \"/content/drive/MyDrive/Eixo_05/dados/\"\n"],"metadata":{"id":"og9kIWhfeOkv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762081144370,"user_tz":0,"elapsed":1750,"user":{"displayName":"Ravi Ferreira Pellizzi","userId":"04678001779669650850"}},"outputId":"490553c5-39da-4a96-b950-8bfb2f790bcd"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Carregar featurizações\n","\n","HTFfeaturizedData   = spark.read.parquet(base_path + \"HTFfeaturizedData\")\n","TFIDFfeaturizedData = spark.read.parquet(base_path + \"TFIDFfeaturizedData\")\n","W2VfeaturizedData   = spark.read.parquet(base_path + \"W2VfeaturizedData\")\n","\n","HTFfeaturizedData.name   = \"HTFfeaturizedData\"\n","TFIDFfeaturizedData.name = \"TFIDFfeaturizedData\"\n","W2VfeaturizedData.name   = \"W2VfeaturizedData\"\n","\n","print(\"Contagens:\",\n","      HTFfeaturizedData.count(),\n","      TFIDFfeaturizedData.count(),\n","      W2VfeaturizedData.count())\n"],"metadata":{"id":"NwwSGxZ6R4b9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762081166489,"user_tz":0,"elapsed":22121,"user":{"displayName":"Ravi Ferreira Pellizzi","userId":"04678001779669650850"}},"outputId":"4abe6f38-0d58-4ebe-b2de-2cb5f55c2d69"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Contagens: 50000 50000 50000\n"]}]},{"cell_type":"code","source":["# Helpers + definindo modelos (LR e SVM)\n","\n","from pyspark.ml.classification import LogisticRegression, LinearSVC, OneVsRest\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n","\n","def fit_with_cv(estimator, grid, train, folds=2):\n","    cv = CrossValidator(\n","        estimator=estimator,\n","        estimatorParamMaps=grid,\n","        evaluator=evaluator,\n","        numFolds=folds,\n","        parallelism=2\n","    )\n","    return cv.fit(train)\n","\n","def train_and_eval(dataset):\n","    # split\n","    train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n","    try: train.name = dataset.name\n","    except: pass\n","\n","    # nº classes (para decidir SVM binário x OneVsRest)\n","    n_classes = dataset.select(\"label\").distinct().count()\n","\n","    # ----- Logistic Regression -----\n","    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n","    lr_grid = (ParamGridBuilder()\n","               .addGrid(lr.maxIter, [30])\n","               .addGrid(lr.regParam, [0.0, 0.01])\n","               .addGrid(lr.elasticNetParam, [0.0, 0.5])\n","               .build())\n","    lr_model = fit_with_cv(lr, lr_grid, train)\n","    lr_acc = evaluator.evaluate(lr_model.transform(test)) * 100.0\n","\n","    # ----- Linear SVC (SVM) -----\n","    svc = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n","    if n_classes > 2:\n","        # Fallback simples e robusto para multiclasse\n","        svm_est = OneVsRest(featuresCol=\"features\", labelCol=\"label\", classifier=svc)\n","        svm_grid = ParamGridBuilder().build()  # sem grid para manter simples\n","        svm_model = fit_with_cv(svm_est, svm_grid, train)\n","    else:\n","        svm_grid = (ParamGridBuilder()\n","                    .addGrid(svc.maxIter, [50])\n","                    .addGrid(svc.regParam, [0.1, 0.01])\n","                    .build())\n","        svm_model = fit_with_cv(svc, svm_grid, train)\n","    svm_acc = evaluator.evaluate(svm_model.transform(test)) * 100.0\n","\n","    feat_name = getattr(dataset, \"name\", \"features\")\n","    rows = [\n","        (\"LogisticRegression\", feat_name, float(f\"{lr_acc:.4f}\")),\n","        (\"LinearSVC\",         feat_name, float(f\"{svm_acc:.4f}\")),\n","    ]\n","    return spark.createDataFrame(rows, [\"Classifier\", \"Featurization\", \"Accuracy\"])\n"],"metadata":{"id":"RI1OqjwegHiP","executionInfo":{"status":"ok","timestamp":1762081166516,"user_tz":0,"elapsed":25,"user":{"displayName":"Ravi Ferreira Pellizzi","userId":"04678001779669650850"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Rodar tudo e ver o melhor\n","\n","results = None\n","for ds in [HTFfeaturizedData, TFIDFfeaturizedData, W2VfeaturizedData]:\n","    r = train_and_eval(ds)\n","    r.show(truncate=False)\n","    results = r if results is None else results.union(r)\n","\n","best = results.orderBy(results.Accuracy.desc()).limit(1)\n","print(\"\\n=== MELHOR COMBO ===\")\n","best.show(truncate=False)\n"],"metadata":{"id":"fZvAiz1Deogz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762081865111,"user_tz":0,"elapsed":698591,"user":{"displayName":"Ravi Ferreira Pellizzi","userId":"04678001779669650850"}},"outputId":"0703c31d-2a65-4423-f5b6-a8d51f8ea77d"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+-----------------+--------+\n","|Classifier        |Featurization    |Accuracy|\n","+------------------+-----------------+--------+\n","|LogisticRegression|HTFfeaturizedData|87.7588 |\n","|LinearSVC         |HTFfeaturizedData|89.0819 |\n","+------------------+-----------------+--------+\n","\n","+------------------+-------------------+--------+\n","|Classifier        |Featurization      |Accuracy|\n","+------------------+-------------------+--------+\n","|LogisticRegression|TFIDFfeaturizedData|88.3446 |\n","|LinearSVC         |TFIDFfeaturizedData|90.2535 |\n","+------------------+-------------------+--------+\n","\n","+------------------+-----------------+--------+\n","|Classifier        |Featurization    |Accuracy|\n","+------------------+-----------------+--------+\n","|LogisticRegression|W2VfeaturizedData|87.1124 |\n","|LinearSVC         |W2VfeaturizedData|86.9306 |\n","+------------------+-----------------+--------+\n","\n","\n","=== MELHOR COMBO ===\n","+----------+-------------------+--------+\n","|Classifier|Featurization      |Accuracy|\n","+----------+-------------------+--------+\n","|LinearSVC |TFIDFfeaturizedData|90.2535 |\n","+----------+-------------------+--------+\n","\n"]}]}]}