{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22911,
     "status": "ok",
     "timestamp": 1762081142614,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "4_TJ7tYC5t7E",
    "outputId": "1efcfa60-af8d-4ad7-9238-b5a719605a0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Spark OK -> 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ETAPA 04: APRENDIZADO DE MÁQUINA - CONFIGURAÇÃO DO AMBIENTE\n",
    "# ================================================================\n",
    "\n",
    "# Fecha qualquer sessão Spark anterior para evitar conflitos de memória\n",
    "# Fundamental antes de iniciar uma nova sessão de ML\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Instalação e configuração do ambiente Java + PySpark\n",
    "# Mesmo setup da etapa de pré-processamento para garantir compatibilidade\n",
    "print(\"Configurando ambiente Java + PySpark para Machine Learning...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-17-jdk-headless -qq\n",
    "!pip -q install -U pyspark==3.5.1\n",
    "\n",
    "# Configuração das variáveis de ambiente necessárias para o Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"PATH\"]  = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Criação de nova sessão Spark otimizada para Machine Learning\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"eixo05-ml\")  # Nome específico para a etapa de ML\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Spark configurado com sucesso para Machine Learning!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1750,
     "status": "ok",
     "timestamp": 1762081144370,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "og9kIWhfeOkv",
    "outputId": "490553c5-39da-4a96-b950-8bfb2f790bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÇÃO DE ACESSO AOS DADOS E VERIFICAÇÃO DO SPARK\n",
    "# ================================================================\n",
    "\n",
    "# Monta o Google Drive para acessar os dados featurizados\n",
    "# force_remount=False evita remontagem desnecessária se já estiver montado\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Verifica se a sessão Spark está ativa, senão cria uma nova\n",
    "# Garante que sempre temos uma sessão válida para o processamento\n",
    "from pyspark.sql import SparkSession\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define o caminho base onde estão os datasets featurizados\n",
    "# Mesmo diretório usado na etapa de pré-processamento\n",
    "base_path = \"/content/drive/MyDrive/Eixo_05/dados/\"\n",
    "\n",
    "print(\"Google Drive e Spark prontos para carregamento dos dados!\")\n",
    "print(f\"Caminho dos datasets: {base_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22121,
     "status": "ok",
     "timestamp": 1762081166489,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "NwwSGxZ6R4b9",
    "outputId": "4abe6f38-0d58-4ebe-b2de-2cb5f55c2d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagens: 50000 50000 50000\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CARREGAMENTO DOS DATASETS FEATURIZADOS\n",
    "# ================================================================\n",
    "\n",
    "# Carrega os três tipos de featurização gerados na etapa anterior\n",
    "# Cada dataset contém as mesmas avaliações com diferentes representações\n",
    "\n",
    "print(\"Carregando datasets featurizados...\")\n",
    "\n",
    "# 1. HTF (Hashing Term Frequency) - Features esparsas baseadas em hashing\n",
    "HTFfeaturizedData = spark.read.parquet(base_path + \"HTFfeaturizedData\")\n",
    "\n",
    "# 2. TF-IDF - Combinação de unigramas e bigramas com pesos IDF\n",
    "TFIDFfeaturizedData = spark.read.parquet(base_path + \"TFIDFfeaturizedData\")\n",
    "\n",
    "# 3. Word2Vec - Embeddings semânticos densos escalados\n",
    "W2VfeaturizedData = spark.read.parquet(base_path + \"W2VfeaturizedData\")\n",
    "\n",
    "# Adiciona nomes aos DataFrames para identificação durante o treinamento\n",
    "# Útil para logging e comparação de resultados\n",
    "HTFfeaturizedData.name = \"HTFfeaturizedData\"\n",
    "TFIDFfeaturizedData.name = \"TFIDFfeaturizedData\"\n",
    "W2VfeaturizedData.name = \"W2VfeaturizedData\"\n",
    "\n",
    "# Verifica se todos os datasets foram carregados corretamente\n",
    "htf_count = HTFfeaturizedData.count()\n",
    "tfidf_count = TFIDFfeaturizedData.count()\n",
    "w2v_count = W2VfeaturizedData.count()\n",
    "\n",
    "print(\"Datasets carregados com sucesso!\")\n",
    "print(f\"HTF: {htf_count:,} registros\")\n",
    "print(f\"TF-IDF: {tfidf_count:,} registros\")\n",
    "print(f\"Word2Vec: {w2v_count:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1762081166516,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "RI1OqjwegHiP"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÇÃO DOS ALGORITMOS DE MACHINE LEARNING E PIPELINE\n",
    "# ================================================================\n",
    "\n",
    "# Imports dos algoritmos de classificação e ferramentas de avaliação\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Configuração do avaliador principal baseado em acurácia\n",
    "# Métrica padrão para comparar performance entre modelos\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "def fit_with_cv(estimator, grid, train, folds=2):\n",
    "    \"\"\"\n",
    "    Treina um modelo usando validação cruzada com grid search\n",
    "    \n",
    "    Args:\n",
    "        estimator: algoritmo de ML a ser treinado\n",
    "        grid: grade de hiperparâmetros para busca\n",
    "        train: dataset de treinamento\n",
    "        folds: número de folds para validação cruzada\n",
    "    \n",
    "    Returns:\n",
    "        modelo treinado com melhores hiperparâmetros\n",
    "    \"\"\"\n",
    "    cv = CrossValidator(\n",
    "        estimator=estimator,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=folds,      # 2 folds para balancear tempo vs robustez\n",
    "        parallelism=2        # Paralelização para acelerar o processo\n",
    "    )\n",
    "    return cv.fit(train)\n",
    "\n",
    "def train_and_eval(dataset):\n",
    "    \"\"\"\n",
    "    Pipeline completo de treinamento e avaliação para um dataset\n",
    "    \n",
    "    Processo:\n",
    "    1. Divide dataset em treino (80%) e teste (20%)\n",
    "    2. Detecta número de classes para escolher estratégia SVM\n",
    "    3. Treina Logistic Regression com grid search\n",
    "    4. Treina Linear SVC (binário) ou OneVsRest (multiclasse)\n",
    "    5. Avalia ambos modelos no conjunto de teste\n",
    "    6. Retorna DataFrame com resultados comparativos\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame com features e labels\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do Spark com resultados de acurácia\n",
    "    \"\"\"\n",
    "    \n",
    "    # DIVISÃO TREINO/TESTE\n",
    "    # Split estratificado com seed fixa para reprodutibilidade\n",
    "    train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Preserva o nome do dataset para logging\n",
    "    try: \n",
    "        train.name = dataset.name\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    # DETECÇÃO DO NÚMERO DE CLASSES\n",
    "    # Necessário para escolher estratégia do SVM\n",
    "    n_classes = dataset.select(\"label\").distinct().count()\n",
    "    print(f\"Processando {getattr(dataset, 'name', 'dataset')}: {n_classes} classes detectadas\")\n",
    "\n",
    "    # ===== LOGISTIC REGRESSION =====\n",
    "    # Algoritmo linear probabilístico, eficiente para classificação de texto\n",
    "    print(\"Treinando Logistic Regression...\")\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Grid search para otimização de hiperparâmetros\n",
    "    lr_grid = (ParamGridBuilder()\n",
    "               .addGrid(lr.maxIter, [30])              # Iterações máximas\n",
    "               .addGrid(lr.regParam, [0.0, 0.01])      # Regularização L2\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5]) # Mistura L1/L2 (0=L2, 1=L1)\n",
    "               .build())\n",
    "    \n",
    "    lr_model = fit_with_cv(lr, lr_grid, train)\n",
    "    lr_predictions = lr_model.transform(test)\n",
    "    lr_acc = evaluator.evaluate(lr_predictions) * 100.0\n",
    "\n",
    "    # ===== LINEAR SVC (SUPPORT VECTOR MACHINE) =====\n",
    "    # Algoritmo baseado em margens, robusto para alta dimensionalidade\n",
    "    print(\"Treinando Linear SVC...\")\n",
    "    \n",
    "    svc = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        # ESTRATÉGIA MULTICLASSE: One-vs-Rest\n",
    "        # Treina um classificador binário para cada classe\n",
    "        print(\"Usando estratégia One-vs-Rest para multiclasse\")\n",
    "        svm_est = OneVsRest(featuresCol=\"features\", labelCol=\"label\", classifier=svc)\n",
    "        svm_grid = ParamGridBuilder().build()  # Grid vazio para simplicidade\n",
    "        svm_model = fit_with_cv(svm_est, svm_grid, train)\n",
    "    else:\n",
    "        # ESTRATÉGIA BINÁRIA: SVM direto\n",
    "        print(\"Usando SVM binário direto\")\n",
    "        svm_grid = (ParamGridBuilder()\n",
    "                    .addGrid(svc.maxIter, [50])       # Iterações máximas\n",
    "                    .addGrid(svc.regParam, [0.1, 0.01]) # Parâmetro de regularização\n",
    "                    .build())\n",
    "        svm_model = fit_with_cv(svc, svm_grid, train)\n",
    "    \n",
    "    svm_predictions = svm_model.transform(test)\n",
    "    svm_acc = evaluator.evaluate(svm_predictions) * 100.0\n",
    "\n",
    "    # COMPILAÇÃO DOS RESULTADOS\n",
    "    # Cria DataFrame com resultados para comparação posterior\n",
    "    feat_name = getattr(dataset, \"name\", \"features\")\n",
    "    results_data = [\n",
    "        (\"LogisticRegression\", feat_name, float(f\"{lr_acc:.4f}\")),\n",
    "        (\"LinearSVC\", feat_name, float(f\"{svm_acc:.4f}\")),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Logistic Regression: {lr_acc:.2f}%\")\n",
    "    print(f\"Linear SVC: {svm_acc:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return spark.createDataFrame(results_data, [\"Classifier\", \"Featurization\", \"Accuracy\"])\n",
    "\n",
    "print(\"Funções de treinamento configuradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 698591,
     "status": "ok",
     "timestamp": 1762081865111,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "fZvAiz1Deogz",
    "outputId": "0703c31d-2a65-4423-f5b6-a8d51f8ea77d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+--------+\n",
      "|Classifier        |Featurization    |Accuracy|\n",
      "+------------------+-----------------+--------+\n",
      "|LogisticRegression|HTFfeaturizedData|87.7588 |\n",
      "|LinearSVC         |HTFfeaturizedData|89.0819 |\n",
      "+------------------+-----------------+--------+\n",
      "\n",
      "+------------------+-------------------+--------+\n",
      "|Classifier        |Featurization      |Accuracy|\n",
      "+------------------+-------------------+--------+\n",
      "|LogisticRegression|TFIDFfeaturizedData|88.3446 |\n",
      "|LinearSVC         |TFIDFfeaturizedData|90.2535 |\n",
      "+------------------+-------------------+--------+\n",
      "\n",
      "+------------------+-----------------+--------+\n",
      "|Classifier        |Featurization    |Accuracy|\n",
      "+------------------+-----------------+--------+\n",
      "|LogisticRegression|W2VfeaturizedData|87.1124 |\n",
      "|LinearSVC         |W2VfeaturizedData|86.9306 |\n",
      "+------------------+-----------------+--------+\n",
      "\n",
      "\n",
      "=== MELHOR COMBO ===\n",
      "+----------+-------------------+--------+\n",
      "|Classifier|Featurization      |Accuracy|\n",
      "+----------+-------------------+--------+\n",
      "|LinearSVC |TFIDFfeaturizedData|90.2535 |\n",
      "+----------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# EXECUÇÃO DO TREINAMENTO E SALVAMENTO DOS MODELOS\n",
    "# ================================================================\n",
    "\n",
    "# Imports para salvamento de modelos\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"Iniciando treinamento dos modelos em todas as featurizações...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Cria diretório para salvar os modelos treinados\n",
    "models_path = base_path + \"modelos/\"\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "\n",
    "# ================================================================\n",
    "# FUNÇÃO PARA TREINAR E SALVAR MODELOS\n",
    "# ================================================================\n",
    "\n",
    "def train_save_and_eval(dataset):\n",
    "    \"\"\"\n",
    "    Pipeline completo de treinamento, salvamento e avaliação para um dataset\n",
    "    \n",
    "    Além do treinamento original, agora salva os modelos treinados\n",
    "    para reutilização posterior na etapa de análise de resultados.\n",
    "    \n",
    "    Processo:\n",
    "    1. Divide dataset em treino (80%) e teste (20%)\n",
    "    2. Detecta número de classes para escolher estratégia SVM\n",
    "    3. Treina Logistic Regression com grid search\n",
    "    4. Salva modelo Logistic Regression otimizado (formato PySpark)\n",
    "    5. Treina Linear SVC (binário) ou OneVsRest (multiclasse)\n",
    "    6. Salva modelo SVC otimizado (formato PySpark)\n",
    "    7. Avalia ambos modelos no conjunto de teste\n",
    "    8. Retorna DataFrame com resultados comparativos\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame com features e labels\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame do Spark com resultados de acurácia\n",
    "    \"\"\"\n",
    "    \n",
    "    # DIVISÃO TREINO/TESTE\n",
    "    # Split estratificado com seed fixa para reprodutibilidade\n",
    "    train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # Preserva o nome do dataset para logging\n",
    "    try: \n",
    "        train.name = dataset.name\n",
    "    except: \n",
    "        pass\n",
    "\n",
    "    # DETECÇÃO DO NÚMERO DE CLASSES\n",
    "    # Necessário para escolher estratégia do SVM\n",
    "    n_classes = dataset.select(\"label\").distinct().count()\n",
    "    dataset_name = getattr(dataset, \"name\", \"features\")\n",
    "    print(f\"Processando {dataset_name}: {n_classes} classes detectadas\")\n",
    "\n",
    "    # ===== LOGISTIC REGRESSION =====\n",
    "    # Algoritmo linear probabilístico, eficiente para classificação de texto\n",
    "    print(\"Treinando Logistic Regression...\")\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    # Grid search para otimização de hiperparâmetros\n",
    "    lr_grid = (ParamGridBuilder()\n",
    "               .addGrid(lr.maxIter, [30])              # Iterações máximas\n",
    "               .addGrid(lr.regParam, [0.0, 0.01])      # Regularização L2\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5]) # Mistura L1/L2 (0=L2, 1=L1)\n",
    "               .build())\n",
    "    \n",
    "    lr_cv_model = fit_with_cv(lr, lr_grid, train)\n",
    "    lr_predictions = lr_cv_model.transform(test)\n",
    "    lr_acc = evaluator.evaluate(lr_predictions) * 100.0\n",
    "    \n",
    "    # SALVAR MODELO LOGISTIC REGRESSION (FORMATO PYSPARK)\n",
    "    # Extrai o melhor modelo da validação cruzada\n",
    "    lr_best_model = lr_cv_model.bestModel\n",
    "    lr_model_path = f\"{models_path}lr_{dataset_name}\"\n",
    "    \n",
    "    # Remove diretório se já existir (PySpark não sobrescreve)\n",
    "    import shutil\n",
    "    if os.path.exists(lr_model_path):\n",
    "        shutil.rmtree(lr_model_path)\n",
    "    \n",
    "    # Salva usando formato nativo do PySpark\n",
    "    lr_best_model.write().overwrite().save(lr_model_path)\n",
    "    print(f\"Logistic Regression salvo em: {lr_model_path}\")\n",
    "\n",
    "    # ===== LINEAR SVC (SUPPORT VECTOR MACHINE) =====\n",
    "    # Algoritmo baseado em margens, robusto para alta dimensionalidade\n",
    "    print(\"Treinando Linear SVC...\")\n",
    "    \n",
    "    svc = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
    "    \n",
    "    if n_classes > 2:\n",
    "        # ESTRATÉGIA MULTICLASSE: One-vs-Rest\n",
    "        # Treina um classificador binário para cada classe\n",
    "        print(\"Usando estratégia One-vs-Rest para multiclasse\")\n",
    "        svm_est = OneVsRest(featuresCol=\"features\", labelCol=\"label\", classifier=svc)\n",
    "        svm_grid = ParamGridBuilder().build()  # Grid vazio para simplicidade\n",
    "        svm_cv_model = fit_with_cv(svm_est, svm_grid, train)\n",
    "    else:\n",
    "        # ESTRATÉGIA BINÁRIA: SVM direto\n",
    "        print(\"Usando SVM binário direto\")\n",
    "        svm_grid = (ParamGridBuilder()\n",
    "                    .addGrid(svc.maxIter, [50])       # Iterações máximas\n",
    "                    .addGrid(svc.regParam, [0.1, 0.01]) # Parâmetro de regularização\n",
    "                    .build())\n",
    "        svm_cv_model = fit_with_cv(svc, svm_grid, train)\n",
    "    \n",
    "    svm_predictions = svm_cv_model.transform(test)\n",
    "    svm_acc = evaluator.evaluate(svm_predictions) * 100.0\n",
    "    \n",
    "    # SALVAR MODELO SVC (FORMATO PYSPARK)\n",
    "    # Extrai o melhor modelo da validação cruzada\n",
    "    svc_best_model = svm_cv_model.bestModel\n",
    "    svc_model_path = f\"{models_path}svc_{dataset_name}\"\n",
    "    \n",
    "    # Remove diretório se já existir (PySpark não sobrescreve)\n",
    "    if os.path.exists(svc_model_path):\n",
    "        shutil.rmtree(svc_model_path)\n",
    "    \n",
    "    # Salva usando formato nativo do PySpark\n",
    "    svc_best_model.write().overwrite().save(svc_model_path)\n",
    "    print(f\"Linear SVC salvo em: {svc_model_path}\")\n",
    "\n",
    "    # COMPILAÇÃO DOS RESULTADOS\n",
    "    # Cria DataFrame com resultados para comparação posterior\n",
    "    feat_name = getattr(dataset, \"name\", \"features\")\n",
    "    results_data = [\n",
    "        (\"LogisticRegression\", feat_name, float(f\"{lr_acc:.4f}\")),\n",
    "        (\"LinearSVC\", feat_name, float(f\"{svm_acc:.4f}\")),\n",
    "    ]\n",
    "    \n",
    "    print(f\"Logistic Regression: {lr_acc:.2f}%\")\n",
    "    print(f\"Linear SVC: {svm_acc:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return spark.createDataFrame(results_data, [\"Classifier\", \"Featurization\", \"Accuracy\"])\n",
    "\n",
    "# Executa treinamento para cada tipo de featurização\n",
    "# Compara sistematicamente todas as combinações modelo + features\n",
    "results = None\n",
    "\n",
    "# Lista dos datasets para processar em ordem de complexidade\n",
    "datasets = [HTFfeaturizedData, TFIDFfeaturizedData, W2VfeaturizedData]\n",
    "dataset_names = [\"HTF (Hashing)\", \"TF-IDF\", \"Word2Vec\"]\n",
    "\n",
    "for i, ds in enumerate(datasets):\n",
    "    print(f\"\\n{i+1}/3 - Processando {dataset_names[i]}...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Treina, salva e avalia ambos algoritmos no dataset atual\n",
    "    r = train_save_and_eval(ds)\n",
    "    \n",
    "    # Mostra resultados imediatos\n",
    "    r.show(truncate=False)\n",
    "    \n",
    "    # Acumula resultados para comparação final\n",
    "    if results is None:\n",
    "        results = r\n",
    "    else:\n",
    "        results = results.union(r)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TREINAMENTO COMPLETO - COMPARAÇÃO FINAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mostra todos os resultados ordenados por acurácia\n",
    "print(\"\\nTodos os resultados (ordenados por acurácia):\")\n",
    "results.orderBy(results.Accuracy.desc()).show(truncate=False)\n",
    "\n",
    "# Identifica e destaca a melhor combinação\n",
    "best = results.orderBy(results.Accuracy.desc()).limit(1)\n",
    "print(\"\\nMELHOR COMBINAÇÃO MODELO + FEATURIZAÇÃO:\")\n",
    "print(\"=\" * 50)\n",
    "best.show(truncate=False)\n",
    "\n",
    "# Coleta dados da melhor combinação para análise\n",
    "best_data = best.collect()[0]\n",
    "best_classifier = best_data['Classifier']\n",
    "best_featurization = best_data['Featurization']\n",
    "best_accuracy = best_data['Accuracy']\n",
    "\n",
    "print(f\"Algoritmo: {best_classifier}\")\n",
    "print(f\"Featurização: {best_featurization}\")\n",
    "print(f\"Acurácia: {best_accuracy:.2f}%\")\n",
    "\n",
    "# ================================================================\n",
    "# SALVAR METADADOS DA MELHOR COMBINAÇÃO (FORMATO JSON)\n",
    "# ================================================================\n",
    "\n",
    "# Salva informações sobre a melhor combinação para uso posterior\n",
    "metadata = {\n",
    "    'best_classifier': best_classifier,\n",
    "    'best_featurization': best_featurization,\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'models_path': models_path,\n",
    "    'all_results': [row.asDict() for row in results.collect()]\n",
    "}\n",
    "\n",
    "# Usa JSON em vez de pickle para compatibilidade\n",
    "metadata_path = f\"{models_path}training_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"\\nMetadados salvos em: {metadata_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODELOS SALVOS COM SUCESSO!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total de modelos salvos: {len(datasets) * 2}\")\n",
    "print(f\"Localização: {models_path}\")\n",
    "print(\"Formato: PySpark ML (nativo) + JSON (metadados)\")\n",
    "print(\"\\nPróxima etapa: Executar 'analise_resultados.ipynb'\")\n",
    "print(\"Os modelos serão carregados automaticamente para análise detalhada.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
