{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bffdcbfc-9501-4d0b-8b4f-361454ea2297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ambiente (Java + PySpark + SparkSession)\n",
    "# Fecha Spark anterior (se houver)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Java + PySpark estáveis para Python 3.12\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-17-jdk-headless -qq\n",
    "!pip -q install -U pyspark==3.5.1\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"PATH\"]  = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"eixo05-preprocess\")\n",
    "         .getOrCreate())\n",
    "print(\"Spark OK ->\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58f5b443-d621-4a30-920a-7edee9bfb0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drive e caminhos\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/Colab Notebooks/spark/\"\n",
    "csv_path  = base_path + \"dataset.csv\"\n",
    "print(\"Base path:\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdc05c43-34aa-401b-9d94-7575984c60e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports e helpers mínimos\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, concat\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, RegexTokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, Word2Vec, MinMaxScaler, NGram, CountVectorizer\n",
    ")\n",
    "\n",
    "def limpar_texto(df: DataFrame, coluna=\"review\"):\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"<[^>]+>\", \"\"))     # remove tags simples\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"[^A-Za-z ]+\", \" \"))# só letras/espaço\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"\\s+\", \" \"))        # colapsa espaços\n",
    "    df = df.withColumn(coluna, lower(col(coluna)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c24ef7bc-73d7-4521-a165-b3480477c052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar CSV e etapas comuns (dropna, label, limpeza, tokens)\n",
    "\n",
    "# 1) Ler e manter apenas colunas usadas\n",
    "reviews = spark.read.csv(csv_path, header=True, escape=\"\\\"\").select(\"sentiment\", \"review\")\n",
    "\n",
    "# 2) Remover nulos essenciais\n",
    "reviews = reviews.na.drop(subset=[\"sentiment\", \"review\"])\n",
    "\n",
    "# 3) Label: sentiment -> label (pula valores inválidos sem travar)\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "df = indexer.fit(reviews).transform(reviews)\n",
    "\n",
    "# 4) Limpeza de texto + tokenização + stopwords\n",
    "df = limpar_texto(df, coluna=\"review\")\n",
    "df = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=r\"\\W+\").transform(df)\n",
    "df = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", caseSensitive=False).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36c63d26-946d-410d-9ad1-736009382812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Featurização HTF + TF-IDF\n",
    "\n",
    "# === HTF (unigramas com hashing) ===\n",
    "htf = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=1<<18)  # opcional: 1<<18\n",
    "htf_df = htf.transform(df)\n",
    "HTFfeaturizedData = (\n",
    "    htf_df.select(\"sentiment\", \"review\", \"label\", \"rawfeatures\")\n",
    "          .withColumnRenamed(\"rawfeatures\", \"features\")\n",
    ")\n",
    "\n",
    "# === TF-IDF (unigramas + bigramas com vocabulário) ===\n",
    "# bigramas\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
    "df_ng = ngram.transform(df)\n",
    "\n",
    "# concatena unigrams + bigrams\n",
    "df_tokens = df_ng.withColumn(\"tokens_12\", concat(\"filtered\", \"bigrams\"))\n",
    "\n",
    "# CountVectorizer com minDF=2 (similar ao sklearn)\n",
    "cv = CountVectorizer(inputCol=\"tokens_12\", outputCol=\"rawfeatures\", minDF=2, vocabSize=1<<18)\n",
    "cv_model = cv.fit(df_tokens)\n",
    "cv_df = cv_model.transform(df_tokens)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(cv_df)\n",
    "TFIDFfeaturizedData = idf_model.transform(cv_df) \\\n",
    "    .select(\"sentiment\", \"review\", \"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e1da902-a39a-424c-8b5b-99e2dd93d01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Featurização Word2Vec (+ escala)\n",
    "w2v = Word2Vec(inputCol=\"filtered\", outputCol=\"features\", vectorSize=250, minCount=5, seed=42)\n",
    "w2v_df = w2v.fit(df).transform(df)\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaled = scaler.fit(w2v_df).transform(w2v_df)\n",
    "\n",
    "W2VfeaturizedData = (\n",
    "    scaled.select(\"sentiment\", \"review\", \"label\", \"scaledFeatures\")\n",
    "          .withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e771429-a6be-4262-84f2-e74b850b6e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvar Parquet + retornar DFs (com nomes)\n",
    "\n",
    "# Salva no Drive\n",
    "HTFfeaturizedData.write.mode(\"overwrite\").parquet(base_path + \"HTFfeaturizedData\")\n",
    "TFIDFfeaturizedData.select(\"sentiment\",\"review\",\"label\",\"features\") \\\n",
    "    .write.mode(\"overwrite\").parquet(base_path + \"TFIDFfeaturizedData\")\n",
    "W2VfeaturizedData.write.mode(\"overwrite\").parquet(base_path + \"W2VfeaturizedData\")\n",
    "\n",
    "# Nomes amigáveis (usados depois no treinamento)\n",
    "HTFfeaturizedData.name   = \"HTFfeaturizedData\"\n",
    "TFIDFfeaturizedData.name = \"TFIDFfeaturizedData\"\n",
    "W2VfeaturizedData.name   = \"W2VfeaturizedData\"\n",
    "\n",
    "print(\"Salvo em:\")\n",
    "print(f\" - {base_path}HTFfeaturizedData\")\n",
    "print(f\" - {base_path}TFIDFfeaturizedData\")\n",
    "print(f\" - {base_path}W2VfeaturizedData\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "processamento_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
