{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7062bc8e-0923-42b7-8960-f2e49633de63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ambiente (Java + PySpark + SparkSession)\n",
    "\n",
    "# Fecha Spark anterior (se houver)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Java + PySpark estáveis para Python 3.12\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-17-jdk-headless -qq\n",
    "!pip -q install -U pyspark==3.5.1\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"PATH\"]  = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"eixo05-preprocess\")\n",
    "         .getOrCreate())\n",
    "print(\"Spark OK ->\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60fc8e6c-1775-42db-b75e-24c0a32c249b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Montar Drive + garantir Spark ativo (sem reinstalar nada)\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "base_path = \"/content/drive/MyDrive/Colab Notebooks/spark/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcec223c-8341-414f-a6b9-c3a726d105e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Carregar featurizações\n",
    "\n",
    "HTFfeaturizedData   = spark.read.parquet(base_path + \"HTFfeaturizedData\")\n",
    "TFIDFfeaturizedData = spark.read.parquet(base_path + \"TFIDFfeaturizedData\")\n",
    "W2VfeaturizedData   = spark.read.parquet(base_path + \"W2VfeaturizedData\")\n",
    "\n",
    "HTFfeaturizedData.name   = \"HTFfeaturizedData\"\n",
    "TFIDFfeaturizedData.name = \"TFIDFfeaturizedData\"\n",
    "W2VfeaturizedData.name   = \"W2VfeaturizedData\"\n",
    "\n",
    "print(\"Contagens:\",\n",
    "      HTFfeaturizedData.count(),\n",
    "      TFIDFfeaturizedData.count(),\n",
    "      W2VfeaturizedData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9e75c78-66b1-4ac2-b640-20e42735943c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helpers + definindo modelos (LR e SVM)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "def fit_with_cv(estimator, grid, train, folds=2):\n",
    "    cv = CrossValidator(\n",
    "        estimator=estimator,\n",
    "        estimatorParamMaps=grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=folds,\n",
    "        parallelism=2\n",
    "    )\n",
    "    return cv.fit(train)\n",
    "\n",
    "def train_and_eval(dataset):\n",
    "    # split\n",
    "    train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "    try: train.name = dataset.name\n",
    "    except: pass\n",
    "\n",
    "    # nº classes (para decidir SVM binário x OneVsRest)\n",
    "    n_classes = dataset.select(\"label\").distinct().count()\n",
    "\n",
    "    # ----- Logistic Regression -----\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "    lr_grid = (ParamGridBuilder()\n",
    "               .addGrid(lr.maxIter, [30])\n",
    "               .addGrid(lr.regParam, [0.0, 0.01])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5])\n",
    "               .build())\n",
    "    lr_model = fit_with_cv(lr, lr_grid, train)\n",
    "    lr_acc = evaluator.evaluate(lr_model.transform(test)) * 100.0\n",
    "\n",
    "    # ----- Linear SVC (SVM) -----\n",
    "    svc = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
    "    if n_classes > 2:\n",
    "        # Fallback simples e robusto para multiclasse\n",
    "        svm_est = OneVsRest(featuresCol=\"features\", labelCol=\"label\", classifier=svc)\n",
    "        svm_grid = ParamGridBuilder().build()  # sem grid para manter simples\n",
    "        svm_model = fit_with_cv(svm_est, svm_grid, train)\n",
    "    else:\n",
    "        svm_grid = (ParamGridBuilder()\n",
    "                    .addGrid(svc.maxIter, [50])\n",
    "                    .addGrid(svc.regParam, [0.1, 0.01])\n",
    "                    .build())\n",
    "        svm_model = fit_with_cv(svc, svm_grid, train)\n",
    "    svm_acc = evaluator.evaluate(svm_model.transform(test)) * 100.0\n",
    "\n",
    "    feat_name = getattr(dataset, \"name\", \"features\")\n",
    "    rows = [\n",
    "        (\"LogisticRegression\", feat_name, float(f\"{lr_acc:.4f}\")),\n",
    "        (\"LinearSVC\",         feat_name, float(f\"{svm_acc:.4f}\")),\n",
    "    ]\n",
    "    return spark.createDataFrame(rows, [\"Classifier\", \"Featurization\", \"Accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b5be5c2-0fb8-4326-a619-f5fef2811cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rodar tudo e ver o melhor\n",
    "\n",
    "results = None\n",
    "for ds in [HTFfeaturizedData, TFIDFfeaturizedData, W2VfeaturizedData]:\n",
    "    r = train_and_eval(ds)\n",
    "    r.show(truncate=False)\n",
    "    results = r if results is None else results.union(r)\n",
    "\n",
    "best = results.orderBy(results.Accuracy.desc()).limit(1)\n",
    "print(\"\\n=== MELHOR COMBO ===\")\n",
    "best.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "aprendizado_maquina",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
