{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15989,
     "status": "ok",
     "timestamp": 1762080485817,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "9eG_u3j1684w",
    "outputId": "c3c3f89e-78da-4a93-d86a-4ee21520dade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Spark OK -> 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ETAPA 03: PRÉ-PROCESSAMENTO - CONFIGURAÇÃO DO AMBIENTE SPARK\n",
    "# ================================================================\n",
    "\n",
    "# Fecha qualquer sessão Spark anterior para evitar conflitos\n",
    "# Importante fazer isso antes de configurar um novo ambiente\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Instalação e configuração do ambiente Java + PySpark\n",
    "# Java 17 é necessário para compatibilidade com PySpark 3.5.1\n",
    "print(\"Configurando ambiente Java + PySpark...\")\n",
    "!apt-get update -qq\n",
    "!apt-get install -y openjdk-17-jdk-headless -qq\n",
    "!pip -q install -U pyspark==3.5.1\n",
    "\n",
    "# Configuração das variáveis de ambiente para Java\n",
    "# JAVA_HOME aponta para a instalação do Java\n",
    "# PATH é atualizado para incluir os binários do Java\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "os.environ[\"PATH\"]  = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "\n",
    "# Criação da sessão Spark\n",
    "# SparkSession é o ponto de entrada para funcionalidades do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"eixo05-preprocess\")  # Nome da aplicação Spark\n",
    "         .getOrCreate())\n",
    "\n",
    "print(\"Spark configurado com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 916,
     "status": "ok",
     "timestamp": 1762080486737,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "ffbAi_I_dhhx",
    "outputId": "b6f1d673-8134-4c5c-c8dc-1df795cc0812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Base path: /content/drive/MyDrive/Eixo_05/dados/\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# CONFIGURAÇÃO DE CAMINHOS E ACESSO AO GOOGLE DRIVE\n",
    "# ================================================================\n",
    "\n",
    "# Monta o Google Drive (força remontagem se necessário)\n",
    "# O Google Drive serve como storage persistente entre sessões\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=False)\n",
    "\n",
    "# Define caminhos para os dados de entrada e saída\n",
    "# base_path: diretório raiz onde ficam todos os dados do projeto\n",
    "# csv_path: arquivo CSV gerado na etapa de coleta de dados\n",
    "base_path = \"/content/drive/MyDrive/Eixo_05/dados/\"\n",
    "csv_path  = base_path + \"dataset.csv\"\n",
    "\n",
    "print(\"Google Drive montado com sucesso!\")\n",
    "print(f\"Caminho base: {base_path}\")\n",
    "print(f\"Arquivo de entrada: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "ok",
     "timestamp": 1762080487151,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "zvpyiFy5dnbI"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# IMPORTS E FUNÇÕES AUXILIARES PARA PRÉ-PROCESSAMENTO DE TEXTO\n",
    "# ================================================================\n",
    "\n",
    "# Imports das bibliotecas PySpark necessárias\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lower, regexp_replace, concat\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer, RegexTokenizer, StopWordsRemover,\n",
    "    HashingTF, IDF, Word2Vec, MinMaxScaler, NGram, CountVectorizer\n",
    ")\n",
    "\n",
    "def limpar_texto(df: DataFrame, coluna=\"review\"):\n",
    "    \"\"\"\n",
    "    Função para limpeza de texto usando PySpark\n",
    "    \n",
    "    Etapas de limpeza:\n",
    "    1. Remove tags HTML (ex: <br>, <p>, etc.)\n",
    "    2. Remove caracteres especiais, mantendo apenas letras e espaços\n",
    "    3. Colapsa múltiplos espaços em um único espaço\n",
    "    4. Converte todo o texto para minúsculas\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame do PySpark\n",
    "        coluna: nome da coluna de texto a ser limpa\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com a coluna de texto limpa\n",
    "    \"\"\"\n",
    "    # Remove tags HTML simples (qualquer texto entre < e >)\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"<[^>]+>\", \"\"))\n",
    "    \n",
    "    # Remove caracteres especiais, mantendo apenas letras e espaços\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"[^A-Za-z ]+\", \" \"))\n",
    "    \n",
    "    # Colapsa múltiplos espaços consecutivos em um único espaço\n",
    "    df = df.withColumn(coluna, regexp_replace(col(coluna), r\"\\s+\", \" \"))\n",
    "    \n",
    "    # Converte tudo para minúsculas para normalização\n",
    "    df = df.withColumn(coluna, lower(col(coluna)))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12994,
     "status": "ok",
     "timestamp": 1762080500150,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "2ANo8jB_dpPr"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# CARREGAMENTO E PRÉ-PROCESSAMENTO INICIAL DOS DADOS\n",
    "# ================================================================\n",
    "\n",
    "# 1) CARREGAMENTO DO DATASET\n",
    "# Lê o CSV gerado na etapa de coleta, selecionando apenas colunas necessárias\n",
    "# escape=\"\\\"\" trata aspas duplas corretamente no CSV\n",
    "print(\"Carregando dataset CSV...\")\n",
    "reviews = spark.read.csv(csv_path, header=True, escape=\"\\\"\").select(\"sentiment\", \"review\")\n",
    "\n",
    "# 2) LIMPEZA DE DADOS NULOS\n",
    "# Remove linhas com valores nulos nas colunas essenciais\n",
    "# Essencial para evitar erros no processamento posterior\n",
    "print(\"Removendo valores nulos...\")\n",
    "reviews = reviews.na.drop(subset=[\"sentiment\", \"review\"])\n",
    "\n",
    "# 3) INDEXAÇÃO DE LABELS\n",
    "# Converte categorias de texto (\"positive\", \"negative\") em índices numéricos\n",
    "# handleInvalid=\"skip\" pula valores inválidos sem travar o processamento\n",
    "print(\"Convertendo labels categóricos para numéricos...\")\n",
    "indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "df = indexer.fit(reviews).transform(reviews)\n",
    "\n",
    "# 4) LIMPEZA DE TEXTO\n",
    "# Aplica a função de limpeza definida anteriormente\n",
    "print(\"Limpando texto das avaliações...\")\n",
    "df = limpar_texto(df, coluna=\"review\")\n",
    "\n",
    "# 5) TOKENIZAÇÃO\n",
    "# Divide o texto em palavras individuais usando regex\n",
    "# pattern=r\"\\W+\" divide por qualquer sequência de caracteres não-alfanuméricos\n",
    "print(\"Tokenizando texto...\")\n",
    "df = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=r\"\\W+\").transform(df)\n",
    "\n",
    "# 6) REMOÇÃO DE STOPWORDS\n",
    "# Remove palavras comuns sem valor semântico (the, and, is, etc.)\n",
    "# caseSensitive=False para tratar palavras independente de maiúsculas/minúsculas\n",
    "print(\"Removendo stopwords...\")\n",
    "df = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", caseSensitive=False).transform(df)\n",
    "\n",
    "print(\"Pré-processamento inicial concluído!\")\n",
    "print(f\"Total de registros processados: {df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 86752,
     "status": "ok",
     "timestamp": 1762080586906,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "lkaZFHIZdq7S"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FEATURIZAÇÃO HTF (HASHING TERM FREQUENCY) E TF-IDF\n",
    "# ================================================================\n",
    "\n",
    "print(\"Iniciando processo de featurização...\")\n",
    "\n",
    "# === FEATURIZAÇÃO HTF (HASHING TERM FREQUENCY) ===\n",
    "# HTF usa função hash para mapear termos em features de tamanho fixo\n",
    "# Vantagens: rápido, não precisa armazenar vocabulário\n",
    "# numFeatures: tamanho do vetor de features (2^18 = ~262k features)\n",
    "print(\"Aplicando HTF (Hashing Term Frequency)...\")\n",
    "htf = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=1<<18)\n",
    "htf_df = htf.transform(df)\n",
    "\n",
    "# Seleciona e renomeia colunas para padronização\n",
    "HTFfeaturizedData = (\n",
    "    htf_df.select(\"sentiment\", \"review\", \"label\", \"rawfeatures\")\n",
    "          .withColumnRenamed(\"rawfeatures\", \"features\")\n",
    ")\n",
    "\n",
    "print(\"HTF aplicado com sucesso!\")\n",
    "\n",
    "# === FEATURIZAÇÃO TF-IDF (TERM FREQUENCY - INVERSE DOCUMENT FREQUENCY) ===\n",
    "# TF-IDF combina frequência de termos com raridade no corpus\n",
    "# Inclui unigramas (palavras individuais) + bigramas (pares de palavras)\n",
    "\n",
    "# 1) GERAÇÃO DE BIGRAMAS\n",
    "# N-gramas capturam relações entre palavras adjacentes\n",
    "# Ex: [\"very\", \"good\"] vira \"very good\"\n",
    "print(\"Gerando bigramas...\")\n",
    "ngram = NGram(n=2, inputCol=\"filtered\", outputCol=\"bigrams\")\n",
    "df_ng = ngram.transform(df)\n",
    "\n",
    "# 2) CONCATENAÇÃO DE UNIGRAMAS + BIGRAMAS\n",
    "# Combina palavras individuais com pares de palavras\n",
    "# Aumenta a expressividade das features\n",
    "print(\"Combinando unigramas e bigramas...\")\n",
    "df_tokens = df_ng.withColumn(\"tokens_12\", concat(\"filtered\", \"bigrams\"))\n",
    "\n",
    "# 3) COUNT VECTORIZER\n",
    "# Constrói vocabulário controlado e conta frequências de termos\n",
    "# minDF=2: remove termos que aparecem em menos de 2 documentos (reduz ruído)\n",
    "# vocabSize: limita tamanho do vocabulário para controle de memória\n",
    "print(\"Construindo vocabulário e contando termos...\")\n",
    "cv = CountVectorizer(inputCol=\"tokens_12\", outputCol=\"rawfeatures\", minDF=2, vocabSize=1<<18)\n",
    "cv_model = cv.fit(df_tokens)\n",
    "cv_df = cv_model.transform(df_tokens)\n",
    "\n",
    "# 4) APLICAÇÃO DO IDF\n",
    "# IDF reduz peso de termos muito comuns e aumenta peso de termos raros\n",
    "# Melhora a capacidade discriminativa das features\n",
    "print(\"Aplicando IDF (Inverse Document Frequency)...\")\n",
    "idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(cv_df)\n",
    "TFIDFfeaturizedData = idf_model.transform(cv_df) \\\n",
    "    .select(\"sentiment\", \"review\", \"label\", \"features\")\n",
    "\n",
    "print(\"TF-IDF aplicado com sucesso!\")\n",
    "print(f\"Tamanho do vocabulário: {len(cv_model.vocabulary):,} termos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 187264,
     "status": "ok",
     "timestamp": 1762080774172,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "ag-8rQ1PduAH"
   },
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# FEATURIZAÇÃO WORD2VEC COM ESCALONAMENTO\n",
    "# ================================================================\n",
    "\n",
    "# === FEATURIZAÇÃO WORD2VEC ===\n",
    "# Word2Vec cria embeddings vetoriais densos que capturam semântica\n",
    "# Palavras similares ficam próximas no espaço vetorial\n",
    "\n",
    "print(\"Aplicando Word2Vec...\")\n",
    "\n",
    "# Parâmetros do Word2Vec:\n",
    "# vectorSize: dimensão dos vetores de embedding (250 dimensões)\n",
    "# minCount: ignora palavras que aparecem menos de 5 vezes (reduz ruído)\n",
    "# seed: garante reprodutibilidade dos resultados\n",
    "w2v = Word2Vec(\n",
    "    inputCol=\"filtered\", \n",
    "    outputCol=\"features\", \n",
    "    vectorSize=250, \n",
    "    minCount=5, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Treina o modelo Word2Vec e aplica transformação\n",
    "w2v_model = w2v.fit(df)\n",
    "w2v_df = w2v_model.transform(df)\n",
    "\n",
    "print(\"Word2Vec aplicado com sucesso!\")\n",
    "\n",
    "# === ESCALONAMENTO MIN-MAX ===\n",
    "# Normaliza os vetores Word2Vec para escala [0,1]\n",
    "# Importante para algoritmos sensíveis à escala (como SVM)\n",
    "\n",
    "print(\"Aplicando escalonamento Min-Max...\")\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "scaler_model = scaler.fit(w2v_df)\n",
    "scaled = scaler_model.transform(w2v_df)\n",
    "\n",
    "# Seleciona colunas finais e renomeia para padronização\n",
    "W2VfeaturizedData = (\n",
    "    scaled.select(\"sentiment\", \"review\", \"label\", \"scaledFeatures\")\n",
    "          .withColumnRenamed(\"scaledFeatures\", \"features\")\n",
    ")\n",
    "\n",
    "print(\"Escalonamento concluído!\")\n",
    "print(f\"Dimensão dos vetores Word2Vec: {w2v.getVectorSize()}\")\n",
    "print(f\"Vocabulário Word2Vec: {len(w2v_model.getVectors().collect()):,} palavras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63193,
     "status": "ok",
     "timestamp": 1762080837371,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "sA6Q32UD_GyX",
    "outputId": "84dc7543-8f19-4a5d-a3ae-3b1f72c36a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo em:\n",
      " - /content/drive/MyDrive/Eixo_05/dados/HTFfeaturizedData\n",
      " - /content/drive/MyDrive/Eixo_05/dados/TFIDFfeaturizedData\n",
      " - /content/drive/MyDrive/Eixo_05/dados/W2VfeaturizedData\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# SALVAMENTO DOS DATASETS FEATURIZADOS\n",
    "# ================================================================\n",
    "\n",
    "print(\"Salvando datasets featurizados em formato Parquet...\")\n",
    "\n",
    "# === SALVAMENTO EM PARQUET ===\n",
    "# Parquet é eficiente para armazenamento de dados estruturados\n",
    "# mode(\"overwrite\") substitui arquivos existentes\n",
    "\n",
    "# Salva dataset com features HTF\n",
    "HTFfeaturizedData.write.mode(\"overwrite\").parquet(base_path + \"HTFfeaturizedData\")\n",
    "print(f\"HTF salvo em: {base_path}HTFfeaturizedData\")\n",
    "\n",
    "# Salva dataset com features TF-IDF\n",
    "# select() garante que apenas colunas necessárias sejam salvas\n",
    "TFIDFfeaturizedData.select(\"sentiment\",\"review\",\"label\",\"features\") \\\n",
    "    .write.mode(\"overwrite\").parquet(base_path + \"TFIDFfeaturizedData\")\n",
    "print(f\"TF-IDF salvo em: {base_path}TFIDFfeaturizedData\")\n",
    "\n",
    "# Salva dataset com features Word2Vec\n",
    "W2VfeaturizedData.write.mode(\"overwrite\").parquet(base_path + \"W2VfeaturizedData\")\n",
    "print(f\"Word2Vec salvo em: {base_path}W2VfeaturizedData\")\n",
    "\n",
    "# === ATRIBUIÇÃO DE NOMES PARA RASTREAMENTO ===\n",
    "# Adiciona atributo 'name' aos DataFrames para identificação posterior\n",
    "# Útil para logging e debugging nas próximas etapas\n",
    "HTFfeaturizedData.name   = \"HTFfeaturizedData\"\n",
    "TFIDFfeaturizedData.name = \"TFIDFfeaturizedData\"\n",
    "W2VfeaturizedData.name   = \"W2VfeaturizedData\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMO DOS DATASETS GERADOS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. HTF (Hashing Term Frequency)\")\n",
    "print(\"   - Features esparsas baseadas em hashing\")\n",
    "print(\"   - Rápido processamento, sem vocabulário armazenado\")\n",
    "print()\n",
    "print(\"2. TF-IDF (Term Frequency - Inverse Document Frequency)\")\n",
    "print(\"   - Combina unigramas + bigramas\")\n",
    "print(\"   - Vocabulário controlado com minDF=2\")\n",
    "print()\n",
    "print(\"3. Word2Vec\")\n",
    "print(\"   - Embeddings semânticos de 250 dimensões\")\n",
    "print(\"   - Escalados com Min-Max [0,1]\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3036,
     "status": "ok",
     "timestamp": 1762080840409,
     "user": {
      "displayName": "Ravi Ferreira Pellizzi",
      "userId": "04678001779669650850"
     },
     "user_tz": 0
    },
    "id": "L1bKm6DG_KIR",
    "outputId": "d8db2b63-bdb1-4bb9-d90e-58a45110792c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagens: 50000 50000 50000\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# VERIFICAÇÃO FINAL DOS DATASETS\n",
    "# ================================================================\n",
    "\n",
    "# Sanity check: verifica se todos os datasets têm o mesmo número de registros\n",
    "# Importante para garantir consistência antes de prosseguir para o ML\n",
    "\n",
    "print(\"Realizando verificação final dos datasets...\")\n",
    "print()\n",
    "\n",
    "# Conta registros em cada dataset\n",
    "htf_count = HTFfeaturizedData.count()\n",
    "tfidf_count = TFIDFfeaturizedData.count()\n",
    "w2v_count = W2VfeaturizedData.count()\n",
    "\n",
    "print(\"CONTAGEM DE REGISTROS:\")\n",
    "print(f\"HTF:     {htf_count:,} registros\")\n",
    "print(f\"TF-IDF:  {tfidf_count:,} registros\") \n",
    "print(f\"Word2Vec: {w2v_count:,} registros\")\n",
    "\n",
    "# Verifica se todas as contagens são iguais\n",
    "if htf_count == tfidf_count == w2v_count:\n",
    "    print(f\"\\nSUCESSO: Todos os datasets têm {htf_count:,} registros!\")\n",
    "    print(\"Os dados estão prontos para a etapa de Machine Learning.\")\n",
    "else:\n",
    "    print(f\"\\nATENÇÃO: Inconsistência no número de registros!\")\n",
    "    print(\"Revisar o processamento antes de prosseguir.\")\n",
    "\n",
    "print(f\"\\nPróxima etapa: Executar 'aprendizado_maquina.ipynb'\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
