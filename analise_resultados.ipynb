{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# CÉLULA 1: DIAGNÓSTICO DE ARQUIVOS E CAMINHOS\n",
    "# ==============================================\n",
    "# Verifica onde estão localizados os dados e modelos\n",
    "# Útil para troubleshooting de caminhos incorretos\n",
    "\n",
    "import os\n",
    "\n",
    "# Monta Google Drive se necessário\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive montado com sucesso!\")\n",
    "except:\n",
    "    print(\"Ambiente local detectado ou Drive já montado\")\n",
    "\n",
    "# Verifica caminhos possíveis\n",
    "possible_paths = [\n",
    "    \"/content/drive/MyDrive/Eixo_05/dados/\",\n",
    "    \"/content/drive/MyDrive/dados/\", \n",
    "    \"/content/dados/\",\n",
    "    \"./dados/\",\n",
    "    \"../dados/\",\n",
    "    \"dados/\",\n",
    "    \"/home/pellizzi/projects/eixo5_grupo2_20252/dados/\"\n",
    "]\n",
    "\n",
    "print(\"\\nVerificando caminhos possíveis:\")\n",
    "for path in possible_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"✓ ENCONTRADO: {path}\")\n",
    "        if os.path.exists(path + \"modelos/\"):\n",
    "            print(f\"  └── ✓ Pasta modelos existe: {path}modelos/\")\n",
    "            # Lista arquivos na pasta modelos\n",
    "            try:\n",
    "                files = os.listdir(path + \"modelos/\")\n",
    "                print(f\"      Arquivos encontrados: {files}\")\n",
    "            except:\n",
    "                print(\"      Erro ao listar arquivos\")\n",
    "        else:\n",
    "            print(f\"  └── ✗ Pasta modelos NÃO existe\")\n",
    "    else:\n",
    "        print(f\"✗ NÃO EXISTE: {path}\")\n",
    "\n",
    "# Lista arquivos no diretório atual para orientação\n",
    "print(f\"\\nArquivos no diretório atual:\")\n",
    "try:\n",
    "    current_files = os.listdir(\".\")\n",
    "    for f in current_files:\n",
    "        print(f\"  {f}\")\n",
    "except:\n",
    "\n",
    "    print(\"Erro ao listar diretório atual\")print(\"=\"*50)\n",
    "\n",
    "print(\"Execute esta célula para identificar o caminho correto!\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d50ZT3baLYB0",
    "outputId": "2d1d4244-2b2c-4e00-f17f-c1e3b41ed330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> Dados carregados: 50000 50000 50000\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# CÉLULA 2: CONFIGURAÇÃO E DETECÇÃO DE CAMINHOS\n",
    "# ================================================\n",
    "# Inicializa Spark e detecta automaticamente onde estão os dados\n",
    "# Verifica se modelos pré-treinados estão disponíveis\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Detecta automaticamente o caminho correto dos dados\n",
    "possible_base_paths = [\n",
    "    \"/content/drive/MyDrive/Eixo_05/dados/\",\n",
    "    \"/content/drive/MyDrive/dados/\", \n",
    "    \"/content/dados/\",\n",
    "    \"./dados/\",\n",
    "    \"../dados/\",\n",
    "    \"dados/\",\n",
    "    \"/home/pellizzi/projects/eixo5_grupo2_20252/dados/\"\n",
    "]\n",
    "\n",
    "base_path = None\n",
    "for path in possible_base_paths:\n",
    "    if os.path.exists(path):\n",
    "        base_path = path\n",
    "        break\n",
    "\n",
    "if base_path is None:\n",
    "    raise Exception(\"Diretório de dados não encontrado. Verifique se os dados estão disponíveis.\")\n",
    "\n",
    "print(f\"Usando caminho: {base_path}\")\n",
    "\n",
    "models_path = base_path + \"modelos/\"\n",
    "if not os.path.exists(models_path):\n",
    "    raise Exception(f\"Modelos não encontrados em {models_path}. Execute primeiro 'aprendizado_maquina.ipynb'\")\n",
    "\n",
    "metadata_path = f\"{models_path}training_metadata.json\"\n",
    "\n",
    "if not os.path.exists(metadata_path):print(\"Modelos encontrados!\")\n",
    "\n",
    "    raise Exception(f\"Metadados não encontrados em {metadata_path}. Execute o treinamento primeiro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CpKu4gGKSpd"
   },
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# CÉLULA 3: CARREGAMENTO DE MODELOS E DATASETS\n",
    "# ===============================================\n",
    "# Carrega metadados do treinamento e modelos salvos\n",
    "# Prepara dataset com a melhor featurização identificada\n",
    "\n",
    "import json\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegressionModel, LinearSVCModel, OneVsRestModel\n",
    "\n",
    "# Carrega metadados\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "best_classifier = metadata['best_classifier']\n",
    "best_featurization = metadata['best_featurization']\n",
    "best_accuracy = metadata['best_accuracy']\n",
    "\n",
    "print(f\"Melhor modelo: {best_classifier} com {best_featurization} ({best_accuracy:.2f}%)\")\n",
    "\n",
    "# Carrega dataset\n",
    "dataset_mapping = {\n",
    "    \"HTFfeaturizedData\": \"HTFfeaturizedData\",\n",
    "    \"TFIDFfeaturizedData\": \"TFIDFfeaturizedData\", \n",
    "    \"W2VfeaturizedData\": \"W2VfeaturizedData\"\n",
    "}\n",
    "\n",
    "optimal_dataset_path = base_path + dataset_mapping[best_featurization]\n",
    "ds = spark.read.parquet(optimal_dataset_path)\n",
    "train, test = ds.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Dataset: {ds.count():,} registros | Treino: {train.count():,} | Teste: {test.count():,}\")\n",
    "\n",
    "# Carrega modelos\n",
    "lr_model_path = f\"{models_path}lr_{best_featurization}\"\n",
    "svc_model_path = f\"{models_path}svc_{best_featurization}\"\n",
    "\n",
    "lr_model = LogisticRegressionModel.load(lr_model_path)\n",
    "\n",
    "try:print(\"Modelos carregados com sucesso!\")\n",
    "\n",
    "    svc_model = OneVsRestModel.load(svc_model_path)\n",
    "\n",
    "except:    svc_model = LinearSVCModel.load(svc_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6saCVXprK0Mt",
    "outputId": "2108baec-fde6-4b69-c7a1-f3e4601f01fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LogisticRegression (TFIDFfeaturizedData) ===\n",
      "Acurácia : 89.16%\n",
      "Taxa erro: 10.84%\n",
      "F1-score : 0.8916\n",
      "Matriz de confusão (label x prediction):\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|0.0  |0.0       |4312 |\n",
      "|0.0  |1.0       |583  |\n",
      "|1.0  |0.0       |490  |\n",
      "|1.0  |1.0       |4516 |\n",
      "+-----+----------+-----+\n",
      "\n",
      "\n",
      "=== LinearSVC (TFIDFfeaturizedData) ===\n",
      "Acurácia : 90.27%\n",
      "Taxa erro: 9.73%\n",
      "F1-score : 0.9027\n",
      "Matriz de confusão (label x prediction):\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|0.0  |0.0       |4354 |\n",
      "|0.0  |1.0       |541  |\n",
      "|1.0  |0.0       |422  |\n",
      "|1.0  |1.0       |4584 |\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CÉLULA 4: AVALIAÇÃO E ANÁLISE INDIVIDUAL\n",
    "# ========================================\n",
    "# Avalia cada modelo carregado com métricas detalhadas\n",
    "# Gera matrizes de confusão para análise visual\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def evaluate_model(name, model, test):\n",
    "    preds = model.transform(test).cache()\n",
    "    \n",
    "    eval_acc = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    eval_f1 = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "    eval_precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "    eval_recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "    \n",
    "    accuracy = eval_acc.evaluate(preds)\n",
    "    f1_score = eval_f1.evaluate(preds)\n",
    "    precision = eval_precision.evaluate(preds)\n",
    "    recall = eval_recall.evaluate(preds)\n",
    "    \n",
    "    print(f\"{name}: Acurácia {accuracy:.3f} | F1 {f1_score:.3f} | Precisão {precision:.3f} | Recall {recall:.3f}\")\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    confusion_data = preds.groupBy(\"label\", \"prediction\").count().collect()\n",
    "    confusion_matrix = [[0, 0], [0, 0]]\n",
    "    for row in confusion_data:\n",
    "        real = int(row['label'])\n",
    "        pred = int(row['prediction'])\n",
    "        count = row['count']\n",
    "        confusion_matrix[real][pred] = count\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negativo', 'Positivo'],\n",
    "                yticklabels=['Negativo', 'Positivo'])\n",
    "    plt.title(f'Matriz de Confusão - {name}')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'model': name,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "\n",
    "# Análise dos modelos\n",
    "print(\"Avaliando modelos...\")\n",
    "modelos = [(\"Logistic Regression\", lr_model), (\"Linear SVC\", svc_model)]\n",
    "resultados = []\n",
    "\n",
    "print(\"Análise concluída!\")\n",
    "\n",
    "for nome, modelo in modelos:\n",
    "\n",
    "    resultado = evaluate_model(nome, modelo, test)    resultados.append(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# CÉLULA 5: COMPARAÇÃO ENTRE MODELOS\n",
    "# =====================================\n",
    "# Compara performance dos modelos lado a lado\n",
    "# Gera gráficos comparativos de métricas\n",
    "\n",
    "df_comparison = pd.DataFrame(resultados)\n",
    "\n",
    "print(\"\\nComparação:\")\n",
    "for _, row in df_comparison.iterrows():\n",
    "    print(f\"{row['model']}: Acurácia {row['accuracy']:.3f} | F1 {row['f1_score']:.3f}\")\n",
    "\n",
    "# Gráfico comparativo simples\n",
    "plt.figure(figsize=(8, 5))\n",
    "metrics = ['accuracy', 'f1_score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    models = df_comparison['model']\n",
    "    values = df_comparison[metric]\n",
    "    plt.bar(models, values, color=['skyblue', 'lightcoral'])\n",
    "    plt.title(f'{metric.capitalize()}')\n",
    "\n",
    "    plt.ylim(0, 1)plt.show()\n",
    "\n",
    "    plt.xticks(rotation=45)plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CÉLULA 6: RESULTADO FINAL E VALIDAÇÃO\n",
    "# ========================================\n",
    "# Identifica o melhor modelo e valida com dados de treinamento\n",
    "# Apresenta conclusão do pipeline otimizado\n",
    "\n",
    "best_model = df_comparison.loc[df_comparison['accuracy'].idxmax()]\n",
    "\n",
    "print(f\"\\nMelhor modelo: {best_model['model']}\")\n",
    "print(f\"Acurácia: {best_model['accuracy']:.3f} ({best_model['accuracy']*100:.1f}%)\")\n",
    "print(f\"F1-Score: {best_model['f1_score']:.3f}\")\n",
    "\n",
    "# Validação com treinamento\n",
    "if best_classifier in best_model['model']:\n",
    "\n",
    "    print(\"Resultado consistente com o treinamento!\")print(\"\\nAnálise concluída. Pipeline otimizado funcionando!\")\n",
    "\n",
    "else:\n",
    "    print(\"Diferença detectada nos resultados.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
